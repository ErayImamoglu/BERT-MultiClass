{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwtemaLueD6Miz1NPbNKFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErayImamoglu/BERT-MultiClass/blob/main/multiclass_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svwDGBAwF0HW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stock ml libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "metadata": {
        "id": "F835R9lxGKXI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "3MGrnNGAGTsp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "08fe0d2e-1f5d-4f76-b43a-5624a6fe7064"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train_set_cost_related (2) (2).csv\")\n",
        "\n",
        "# Filter rows where 'CostRelated' is 1\n",
        "filtered_df = df[df['CostRelated'] == 1]\n",
        "\n",
        "# Create the 'list' column based on 4th, 5th, 6th, and 7th columns\n",
        "filtered_df['list'] = filtered_df[filtered_df.columns[4:8]].values.tolist()\n",
        "\n",
        "# Create a new dataframe with only 'Text' and 'list' columns\n",
        "new_df = filtered_df[['Text', 'list']].copy()\n",
        "new_df"
      ],
      "metadata": {
        "id": "GN0svlXMGUZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "WGbOaEezHJon"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.title = dataframe.Text\n",
        "    self.targets = self.data.list\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.title)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    title = str(self.title[index])\n",
        "    title = \" \".join(title.split())\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        title,\n",
        "        None,\n",
        "        add_special_tokens = True,\n",
        "        truncation = True,\n",
        "        max_length = self.max_len,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids = True\n",
        "    )\n",
        "    ids = inputs ['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "    return{\n",
        "        'ids': torch.tensor(ids, dtype=torch.long),\n",
        "        'mask': torch.tensor(mask, dtype=torch.long),\n",
        "        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "    }"
      ],
      "metadata": {
        "id": "fhroVHiOH1uf"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-JQWmZQMSpf",
        "outputId": "f8f6ca64-ddac-4469-f8df-c1885c2830a5"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (183, 2)\n",
            "TRAIN Dataset: (146, 2)\n",
            "TEST Dataset: (37, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "# Fetch and print out the first example to check tokenization\n",
        "first_item = training_set[0]\n",
        "print(f\"Token IDs: {first_item['ids']}\")\n",
        "print(f\"Attention Mask: {first_item['mask']}\")\n",
        "print(f\"Token Type IDs: {first_item['token_type_ids']}\")\n",
        "print(f\"Target: {first_item['targets']}\")"
      ],
      "metadata": {
        "id": "vhudFExZaekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "ZSDcRWlOMrkJ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
        "import transformers\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased') #pre-trained BERT Model - will serve as the backbone for feature extraction.\n",
        "        self.l2 = torch.nn.Dropout(0.3) #Regularization purpose #helps to prevent overfitting during training.\n",
        "        self.l3 = torch.nn.Linear(768, 4) #Classification Purpose - fully connected layer - 4-class classification problem\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids).last_hidden_state[:, 0, :]\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "epUFCACEOkNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "metadata": {
        "id": "slDjZf1gPdIe"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "WdWFLyA8PfE_"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "NiDTfVOEPgI2"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if torch.isnan(param).any():\n",
        "        print(f'Nan in {name}')\n"
      ],
      "metadata": {
        "id": "FMKCT0ohPik_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    fin_sentences = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype=torch.long)\n",
        "            mask = data['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype=torch.float)\n",
        "            sentences = [tokenizer.decode(i, skip_special_tokens=True) for i in ids.cpu().numpy()]\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "            fin_sentences.extend(sentences)\n",
        "    return fin_outputs, fin_targets, fin_sentences\n"
      ],
      "metadata": {
        "id": "BxI4EBKiPj0f"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets, sentences = validation(epoch)\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\n",
        "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "183YIabjbsjg",
        "outputId": "37a938e2-da26-4025-9ed8-47855dab689d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.8378378378378378\n",
            "F1 Score (Micro) = 0.8799999999999999\n",
            "F1 Score (Macro) = 0.8840758192332405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"Accommodation&Transportation\", \"AdministrativeCost\", \"Office_Equipment_Labs\", \"Personnel\"]\n",
        "\n",
        "for sentence, output, target in zip(sentences, outputs, targets):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Predicted Labels:\", [column_names[i] for i, val in enumerate(output) if val == 1])\n",
        "    print(\"True Labels:\", [column_names[i] for i, val in enumerate(target) if val == 1])\n",
        "    print(\"------------\")"
      ],
      "metadata": {
        "id": "3cuZNh0IdOuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "mcm = multilabel_confusion_matrix(targets, outputs)\n",
        "\n",
        "for i, (label_mcm) in enumerate(mcm):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.set(font_scale=1.2)  # for label size\n",
        "    sns.heatmap(label_mcm, annot=True, fmt=\"g\", cmap=\"Blues\")  # font size\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix for Label {i}')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "W_xkAnVUgmdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "np9UeFr4xbiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}